{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing spacy analysis library\n",
    "import spacy\n",
    "#loading the languace dictionary\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nuthin', 'N.C.', 'it', 'is', ':-*', 'Ma’am', 'v_v', \":'(\", 'Rev.', 'q.', '’’', '<333', 'v.v', \"'s\", 'somethin', ':-))', '10', '(._.)', \"nuthin'\", 'Ai', 'Tenn.', ':]', 'Ariz.', 'Ind.', ';-D', 'Conn.', 'Did', \"Nothin'\", 'was', 'ol', ':’)', 'May', \"'ve\", 'ಠ_ಠ', 'ä.', 'y.', 'Nebr.', ';)', '’ll', 'goin’', '-_-', 'Sen.', 'O_o', 'r.', 's.', ':-]', \"Somethin'\", 'Ol']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assign the word in the vocabulary to an array so that we can use that\n",
    "words = [t.text for t in nlp.vocab]\n",
    "print(words[::10])\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inserting new element into the vocabulary\n",
    "doc = nlp(\"\\\"Let's go to N.Y!\\\"\")\n",
    "#if we now look at the length of the vocabulary we will see a growth\n",
    "len(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!', '\"', 'N.Y', 'go', 'to'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can now see what are the new words doing so\n",
    "words2 = [t.text for t in nlp.vocab]\n",
    "set(words2)-set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over tokenizate elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y\n",
      "!\n",
      "\"\n",
      "[\", Let, 's, go, to, N.Y, !, \"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s go"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in doc:\n",
    "    print(t)\n",
    "\n",
    "    \n",
    "#or we can do so by obtaining a list\n",
    "tokens = list(doc)\n",
    "print(tokens)\n",
    "\n",
    "#the doc object can also be indexed i.e doc[5] returns the 5th token\n",
    "doc[5]\n",
    "#this is not a string but a Token object, we can get the string by .text\n",
    "doc[5].text\n",
    "\n",
    "#we can also use slicing [i:j] to get to the ith element to the jth \n",
    "doc[2:4]\n",
    "\n",
    "#we can not reassign tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Let\\'s', 'go', 'to', 'N.Y!\"']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"\\\"Let's go to N.Y!\\\"\"\n",
    "s.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is very useful since we cannot divide the verbs if the form is shorted, whereas using spacy we can distinguish the meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go -> go\n",
      "goes -> goe\n",
      "went -> went\n",
      "wish -> wish\n",
      "wishes -> wish\n",
      "wished -> wish\n",
      "runner -> runner\n",
      "ran -> ran\n",
      "running -> run\n",
      "did -> did\n",
      "does -> doe\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "words = ['go','goes','went','wish','wishes','wished','runner','ran','running', 'did', 'does']\n",
    "\n",
    "for w in words:\n",
    "    print(\"{} -> {}\".format(w, p_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 2\n",
    "\n",
    "Stemming doesn't always work because there are some irregular forms wich does not follow grammar rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -> -PRON-\n",
      "will -> will\n",
      "meet -> meet\n",
      "you -> -PRON-\n",
      "in -> in\n",
      "the -> the\n",
      "meeting -> meeting\n",
      "after -> after\n",
      "the -> the\n",
      "meeting -> meeting\n",
      "after -> after\n",
      "meeting -> meet\n",
      "the -> the\n",
      "runner -> runner\n",
      "that -> that\n",
      "will -> will\n",
      "run -> run\n",
      "away -> away\n",
      "because -> because\n",
      "his -> -PRON-\n",
      "mother -> mother\n",
      "ran -> run\n",
      "when -> when\n",
      "he -> -PRON-\n",
      "was -> be\n",
      "born -> bear\n",
      "and -> and\n",
      "did -> do\n",
      "that -> that\n",
      "so -> so\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "st = nlp(\"I will meet you in the meeting after the meeting after meeting the runner that will run away because his mother ran when he was born and did that so.\")\n",
    "for t in st:\n",
    "    print(\"{} -> {}\".format(t.text,t.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 3\n",
    "As we can see the lemmatizer is thousand times better than the stemmatizer since it does a sort of analysis and is able to get even the irregular forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'otherwise', 'hers', 'would', 'back', 'call', 'against', 'how', 'towards', 'others']\n",
      "I -> True\n",
      "will -> True\n",
      "meet -> False\n",
      "you -> True\n",
      "in -> True\n",
      "the -> True\n",
      "meeting -> False\n",
      "after -> True\n",
      "the -> True\n",
      "meeting -> False\n",
      "after -> True\n",
      "meeting -> False\n",
      "the -> True\n",
      "runner -> False\n",
      "that -> True\n",
      "will -> True\n",
      "run -> False\n",
      "away -> False\n",
      "because -> True\n",
      "his -> True\n",
      "mother -> False\n",
      "ran -> False\n",
      "when -> True\n",
      "he -> True\n",
      "was -> True\n",
      "born -> False\n",
      "and -> True\n",
      "did -> True\n",
      "that -> True\n",
      "so -> True\n",
      ". -> False\n"
     ]
    }
   ],
   "source": [
    "#we can access the stop words of a vocabulary as follows\n",
    "print(list(nlp.Defaults.stop_words)[:10])\n",
    "\n",
    "#we can check if a word is a stop word\n",
    "\"the\" in nlp.Defaults.stop_words\n",
    "\n",
    "#to make things faster we can use this sintax\n",
    "for t in st:\n",
    "    print(\"{} -> {}\".format(t, t.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to remove stop words (if we need to) we can use\n",
    "nlp.Defaults.stop_words.remove('go')\n",
    "\n",
    "#Also we can add stop words as follows\n",
    "nlp.Defaults.stop_words.add('!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 4\n",
    "\n",
    "We want to get rid of very frequent words because, as information theory says, they have very low content of information and thus are not very useful to us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguish Part Of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -> PRON\n",
      "will -> VERB\n",
      "meet -> VERB\n",
      "you -> PRON\n",
      "in -> ADP\n",
      "the -> DET\n",
      "meeting -> NOUN\n",
      "after -> ADP\n",
      "the -> DET\n",
      "meeting -> NOUN\n",
      "after -> ADP\n",
      "meeting -> VERB\n",
      "the -> DET\n",
      "runner -> NOUN\n",
      "that -> DET\n",
      "will -> VERB\n",
      "run -> VERB\n",
      "away -> ADV\n",
      "because -> SCONJ\n",
      "his -> DET\n",
      "mother -> NOUN\n",
      "ran -> VERB\n",
      "when -> ADV\n",
      "he -> PRON\n",
      "was -> AUX\n",
      "born -> VERB\n",
      "and -> CCONJ\n",
      "did -> AUX\n",
      "that -> DET\n",
      "so -> ADV\n",
      ". -> PUNCT\n"
     ]
    }
   ],
   "source": [
    "for t in st:\n",
    "    print(\"{} -> {}\".format(t, t.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -> pronoun, personal\n",
      "will -> verb, modal auxiliary\n",
      "meet -> verb, base form\n",
      "you -> pronoun, personal\n",
      "in -> conjunction, subordinating or preposition\n",
      "the -> determiner\n",
      "meeting -> noun, singular or mass\n",
      "after -> conjunction, subordinating or preposition\n",
      "the -> determiner\n",
      "meeting -> noun, singular or mass\n",
      "after -> conjunction, subordinating or preposition\n",
      "meeting -> verb, gerund or present participle\n",
      "the -> determiner\n",
      "runner -> noun, singular or mass\n",
      "that -> wh-determiner\n",
      "will -> verb, modal auxiliary\n",
      "run -> verb, base form\n",
      "away -> adverb\n",
      "because -> conjunction, subordinating or preposition\n",
      "his -> pronoun, possessive\n",
      "mother -> noun, singular or mass\n",
      "ran -> verb, past tense\n",
      "when -> wh-adverb\n",
      "he -> pronoun, personal\n",
      "was -> verb, past tense\n",
      "born -> verb, past participle\n",
      "and -> conjunction, coordinating\n",
      "did -> verb, past tense\n",
      "that -> determiner\n",
      "so -> adverb\n",
      ". -> punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "#we can obtain more information as\n",
    "for t in st:\n",
    "    print(\"{} -> {}\".format(t.text, spacy.explain(t.tag_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 5\n",
    "\n",
    "The difference is that one is faster to read and less informative, whereas the other is slower but more informative. An useful application for the coarse grained tags is a grammatical analysis program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "It allows us to recognize a company, character and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Silvio Berlusconi, EU, Italy, Amazon.com)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = nlp(\"Silvio Berlusconi is to offer EU leaders a historic grand bargain on Italy and Brexit- he is the owner of Amazon.com\")\n",
    "cp. ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silvio Berlusconi -> Silvio Berlusconi - PERSON - People, including fictional\n",
      "EU -> EU - ORG - Companies, agencies, institutions, etc.\n",
      "Italy -> Italy - GPE - Countries, cities, states\n",
      "Amazon.com -> Amazon.com - ORG - Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "for t in cp.ents:\n",
    "    print(\"{} -> {} - {} - {}\".format(t, t.text, t.label_, spacy.explain(t.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy has a visualizer for ents labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Silvio Berlusconi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is to offer \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    EU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " leaders a historic grand bargain on \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Italy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and Brexit- he is the owner of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(cp, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 6\n",
    "\n",
    "POS and NER are not the same thing. Knowing that a word is a noun and knowing that a word is a noun wich represents a politician, a company or an organization is totatally different and less informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -> True\n",
      "ai -> None\n",
      "n't -> None\n",
      "mad -> None\n",
      ". -> None\n",
      "I'mm -> True\n",
      "just -> None\n",
      "asking -> None\n",
      "why -> None\n",
      "the -> None\n",
      "fuck -> None\n",
      "you -> None\n",
      "do -> None\n",
      "n't -> None\n",
      "answer -> None\n",
      "fans -> None\n",
      "? -> None\n",
      "You -> True\n",
      "could -> None\n",
      "have -> None\n",
      "signed -> None\n",
      "an -> None\n",
      "autograph -> None\n",
      "for -> None\n",
      "Matthew -> None\n"
     ]
    }
   ],
   "source": [
    "fr = nlp(\"I ain't mad. I'mm just asking why the fuck you don't answer fans? You could have signed an autograph for Matthew\")\n",
    "list(fr.sents)\n",
    "\n",
    "#we can check if a token is the first token of a sequence as follows\n",
    "for t in fr:\n",
    "    print(\"{} -> {}\".format(t.text, t.is_sent_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I ain't mad\",\n",
       " \" I'mm just asking why the fuck you don't answer fans? You could have signed an autograph for Matthew\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"I ain't mad. I'mm just asking why the fuck you don't answer fans? You could have signed an autograph for Matthew\"\n",
    "x.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it's not the same thing. It does not just not split the sentence with other punctuation but if we have some number it would have split the phrase giving it no sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the csv file with pandas as DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "spam = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "\n",
    "#we are going to use just the first two columns\n",
    "spam = spam[['v1','v2']]\n",
    "spam = spam.rename(columns={'v1':'class', 'v2':'text'})\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham ---- Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
     ]
    }
   ],
   "source": [
    "#Inspect some information\n",
    "print(spam.iloc[0]['class'], '----', spam.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class                                               text\n",
      "5062   ham               Ok i also wan 2 watch e 9 pm show...\n",
      "39     ham  Hello! How's you and how did saturday go? I wa...\n",
      "4209   ham  No da:)he is stupid da..always sending like th...\n",
      "4500   ham                              So wat's da decision?\n",
      "3578   ham  Multiply the numbers independently and count d...\n",
      "     class                                               text\n",
      "1537   ham  All sounds good. Fingers . Makes it difficult ...\n",
      "963    ham  Yo chad which gymnastics class do you wanna ta...\n",
      "4421   ham            MMM ... Fuck .... Merry Christmas to me\n",
      "46     ham      Didn't you get hep b immunisation in nigeria.\n",
      "581    ham     Ok anyway no need to change with what you said\n"
     ]
    }
   ],
   "source": [
    "#we can split the dataset in order to work with machine learning as follows\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234) #never put the seed, just for instructionally purpose\n",
    "train_set, test_set = train_test_split(spam, test_size=0.25) #25% for test set\n",
    "print(train_set.head())\n",
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "class    5572 non-null object\n",
      "text     5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "spam.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4179 entries, 5062 to 2863\n",
      "Data columns (total 2 columns):\n",
      "class    4179 non-null object\n",
      "text     4179 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 97.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1393 entries, 1537 to 4118\n",
      "Data columns (total 2 columns):\n",
      "class    1393 non-null object\n",
      "text     1393 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 32.6+ KB\n"
     ]
    }
   ],
   "source": [
    "test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "------------------\n",
      "     class                                               text\n",
      "5062   ham               Ok i also wan 2 watch e 9 pm show...\n",
      "39     ham  Hello! How's you and how did saturday go? I wa...\n",
      "4209   ham  No da:)he is stupid da..always sending like th...\n",
      "4500   ham                              So wat's da decision?\n",
      "3578   ham  Multiply the numbers independently and count d...\n",
      "-------------\n",
      "     class                                               text\n",
      "1537   ham  All sounds good. Fingers . Makes it difficult ...\n",
      "963    ham  Yo chad which gymnastics class do you wanna ta...\n",
      "4421   ham            MMM ... Fuck .... Merry Christmas to me\n",
      "46     ham      Didn't you get hep b immunisation in nigeria.\n",
      "581    ham     Ok anyway no need to change with what you said\n"
     ]
    }
   ],
   "source": [
    "print(spam.head())\n",
    "print('------------------')\n",
    "print(train_set.head())\n",
    "print('-------------')\n",
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 8\n",
    "\n",
    "The indexes have been randomly sorted in order to reduce the possibility that the algorithm will work for every input and not just for those of the train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how many elements belong to each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "train_set.groupby('class').count().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 9\n",
    "\n",
    "This is something we have to check in order to be able to give the algorithm a good accuracy because if we have many examples from a class and very low examples of the other class it could say that everything is just from the biggest class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 10\n",
    "\n",
    "x_train is represented as a sparse matrix because we will only have a small amount of non zero elements for each word. If we'd represent the dense matrix we'd need an nxn matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 11\n",
    "\n",
    "This is not a good approach, in fact since our dataset is very unbalanced it could deviate our accuracy. If we'd take a classifier which classifies messages as ham we'd still have a good accuracy since our dataset is unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 12\n",
    "\n",
    "We are learning that we have rightly classified 1188 + 130 emails but we got wrong in 75 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 14\n",
    "\n",
    "The 1NN worked better than the 5NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 15\n",
    "\n",
    "It seems that the MAP works the best for this dataset. The margin is actually large since we have a 10% increase. This means that both recall and precision are higher in value, so the algorithm works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizating and counting words with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is [[1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer() #an object which automatically create a bag of word representation\n",
    "\n",
    "#we can add the vocabulary using the fit method\n",
    "\n",
    "count_vect.fit(['this is', 'a list of', 'short messages'])\n",
    "count_vect.vocabulary_ #a dictionary which maps each word to a unique identifier. It has also removed stop words\n",
    "\n",
    "#transform text using transform\n",
    "features = count_vect.transform(['this is', 'a list of', 'short messages'])\n",
    "# it gives a sparse matrix where each row corresponds to a document and avery index represent the index of the word\n",
    "features = features.todense()\n",
    "print('this is', features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7398"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_count = count_vect.fit_transform(train_set['text'])\n",
    "len(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighbor and MAP Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(train_count, train_set['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FREE MSG:We billed your mobile number by mistake from shortcode 83332.Please call 08081263000 to have charges refunded.This call will be free from a BT landline']\n"
     ]
    }
   ],
   "source": [
    "message = test_set.iloc[260]['text']\n",
    "print([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = count_vect.transform(test_set['text'])\n",
    "y_test = knn.predict(x_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = knn.score(x_test, test_set['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9461593682699211"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96940024, 0.7761194 ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "score = f1_score(test_set['class'], y_test, average=None, labels=['ham','spam'])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8727598238915582"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99204022, 0.95238095])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_count, train_set['class'])\n",
    "x_test_nb = nb.predict(x_test)\n",
    "score_nav = f1_score(test_set['class'], x_test_nb, average=None, labels=['ham','spam'])\n",
    "score_nav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.972210585113811"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_nav.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight words frequency with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97061224, 0.78571429])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "c_vect = CountVectorizer()\n",
    "tf_transformer = TfidfTransformer()\n",
    "\n",
    "train_count = c_vect.fit_transform(train_set['text'])\n",
    "tf_train_count = tf_transformer.fit_transform(train_count)\n",
    "\n",
    "test_count = c_vect.transform(test_set['text'])\n",
    "tf_test_count = tf_transformer.fit_transform(test_count)\n",
    "\n",
    "classifier  = KNeighborsClassifier(n_neighbors=1)\n",
    "classifier.fit(tf_train_count, train_set['class'])\n",
    "\n",
    "y_tf_pred = classifier.predict(tf_test_count)\n",
    "f1_scores = f1_score(test_set['class'], y_tf_pred, average=None, labels=['ham', 'spam'])\n",
    "f1_scores.mean()\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domanda 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97538966, 0.82758621])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "c_vect = CountVectorizer()\n",
    "tf_transformer = TfidfTransformer()\n",
    "\n",
    "train_count = c_vect.fit_transform(train_set['text'])\n",
    "tf_train_count = tf_transformer.fit_transform(train_count)\n",
    "\n",
    "test_count = c_vect.transform(test_set['text'])\n",
    "tf_test_count = tf_transformer.fit_transform(test_count)\n",
    "\n",
    "classifier  = MultinomialNB()\n",
    "classifier.fit(tf_train_count, train_set['class'])\n",
    "\n",
    "y_tf_pred = classifier.predict(tf_test_count)\n",
    "f1_scores = f1_score(test_set['class'], y_tf_pred, average=None, labels=['ham', 'spam'])\n",
    "f1_scores.mean()\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the best results using the word count with the multinomial classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ok also',\n",
       " 'also wan',\n",
       " 'wan watch',\n",
       " 'watch pm',\n",
       " 'pm show',\n",
       " 'hello how',\n",
       " 'how you',\n",
       " 'you and',\n",
       " 'and how',\n",
       " 'how did']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(2,2))\n",
    "count_vect.fit(train_set['text'])\n",
    "list(count_vect.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96940024, 0.7761194 ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "x_train = count_vect.fit_transform(train_set['text'])\n",
    "x_test = count_vect.transform(test_set['text'])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train, train_set['class'])\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "f1score = f1_score(test_set['class'], y_test, average=None, labels=['ham','spam'])\n",
    "f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn doesn't provide tool for stemming etc. we need to\n",
    "#create a custo countvectorizer\n",
    "\n",
    "class POSTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "    def __call__(self, doc):\n",
    "        return [t.pos_ for t in self.nlp(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTJ', 'NOUN', 'PUNCT', 'ADV', 'AUX', 'PRON', 'PUNCT']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = POSTokenizer()\n",
    "tokenizer(\"Hi motherfucker, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94273504, 0.69955157])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we ca create the countvectorizer as follows\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer = POSTokenizer())\n",
    "\n",
    "x_train = count_vect.fit_transform(train_set['text'])\n",
    "x_test = count_vect.transform(test_set['text'])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train, train_set['class'])\n",
    "\n",
    "y_test = classifier.predict(x_test)\n",
    "\n",
    "f1score = f1_score(test_set['class'], y_test, average=None, labels=['ham','spam'])\n",
    "\n",
    "f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 18)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now have 18 features (POS)\n",
    "x_train.shape\n",
    "\n",
    "#let's combine those with the previous word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 7398) (4179, 7416)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "x_train_counts = count_vect.fit_transform(train_set['text'])\n",
    "x_train = hstack([x_train_counts, x_train])\n",
    "print(x_train_counts.shape, x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "    def __call__(self, doc):\n",
    "        return [t.pos_ for t in self.nlp(doc)]\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "count_vect_pos = CountVectorizer(tokenizer=POSTokenizer())\n",
    "\n",
    "x_train_count = count_vect.fit_transform(train_set['text']) #FACCIAMO FIT IN MODO DA INSERIRE LE PAROLE NEL VOCABOLARIO\n",
    "x_train_pos = count_vect_pos.fit_transform(train_set['text'])\n",
    "\n",
    "x_test_count = count_vect.transform(test_set['text'])\n",
    "x_test_pos = count_vect_pos.transform(test_set['text'])\n",
    "\n",
    "x_train = hstack([x_train_count, x_train_pos])\n",
    "x_test = hstack([x_test_count, x_test_pos])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train, train_set['class'])\n",
    "\n",
    "y_test = classifier.predict(x_test)\n",
    "f1score = f1_score(test_set['class'], y_test, average=None, labels=['ham','spam'])\n",
    "f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
